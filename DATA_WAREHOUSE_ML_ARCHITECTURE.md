# üèóÔ∏è UTP CONTROL - Data Warehouse & ML Architecture

**Fecha**: 2026-01-29  
**Estado**: üìã **Especificaci√≥n Completa - Arquitectura Avanzada**

---

## üß† Principio Silicon Valley

> "El Data Warehouse no sirve para guardar datos. Sirve para explicar qu√© est√° pasando."

**Objetivo**: Una sola fuente de verdad, lista para decisiones estrat√©gicas, BI, ML y simulaciones.

---

## üèóÔ∏è Arquitectura General (Alto Nivel)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ FUENTES OPERATIVAS                              ‚îÇ
‚îÇ ‚Ä¢ App M√≥vil del Gestor                          ‚îÇ
‚îÇ ‚Ä¢ Sistema Web (Dashboard)                       ‚îÇ
‚îÇ ‚Ä¢ Cat√°logos (Territorio, Gestores, UP)          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ETL / ELT PIPELINE                              ‚îÇ
‚îÇ ‚Ä¢ Extracci√≥n de datos operativos               ‚îÇ
‚îÇ ‚Ä¢ Transformaci√≥n y limpieza                     ‚îÇ
‚îÇ ‚Ä¢ Validaci√≥n de calidad                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ODS (Operational Data Store)                    ‚îÇ
‚îÇ ‚Ä¢ Datos operativos en tiempo casi real         ‚îÇ
‚îÇ ‚Ä¢ Staging area para transformaciones           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ DATA WAREHOUSE (Star Schema)                    ‚îÇ
‚îÇ ‚Ä¢ fact_visits (tabla de hechos principal)      ‚îÇ
‚îÇ ‚Ä¢ Dimensiones: gestor, territory, date, alert  ‚îÇ
‚îÇ ‚Ä¢ Data Marts: daily_performance, monthly_summary‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ CAPA DE CONSUMO                                 ‚îÇ
‚îÇ ‚Ä¢ BI Dashboards (CEO, Coordinador)             ‚îÇ
‚îÇ ‚Ä¢ ML Models (Predicci√≥n de Riesgo)             ‚îÇ
‚îÇ ‚Ä¢ Simulador What-If                             ‚îÇ
‚îÇ ‚Ä¢ Alertas Predictivas                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## ‚≠ê Star Schema - Dise√±o Can√≥nico

### üéØ **Tabla de Hechos Principal: `fact_visits`**

**Grano**: 1 fila = 1 visita realizada o programada

```sql
CREATE TABLE fact_visits (
    visit_id            BIGSERIAL PRIMARY KEY,
    
    -- Foreign Keys (Dimensiones)
    gestor_id           INTEGER NOT NULL REFERENCES dim_gestor(gestor_id),
    territory_id        INTEGER NOT NULL REFERENCES dim_territory(territory_id),
    productive_unit_id  INTEGER NOT NULL REFERENCES dim_productive_unit(unit_id),
    date_id             INTEGER NOT NULL REFERENCES dim_date(date_id),
    alert_id            INTEGER REFERENCES dim_alert(alert_id),
    
    -- Flags de Estado
    planned_flag        BOOLEAN NOT NULL DEFAULT FALSE,
    executed_flag       BOOLEAN NOT NULL DEFAULT FALSE,
    coverage_flag       BOOLEAN NOT NULL DEFAULT FALSE,
    risk_flag           BOOLEAN NOT NULL DEFAULT FALSE,
    
    -- M√©tricas Cuantitativas
    execution_time_min  INTEGER,              -- Duraci√≥n en minutos
    visit_score         DECIMAL(5,2),         -- Score de calidad (0-100)
    alert_count         INTEGER DEFAULT 0,    -- N√∫mero de alertas generadas
    distance_km         DECIMAL(8,2),         -- Distancia recorrida
    
    -- Timestamps
    planned_datetime    TIMESTAMP,
    executed_datetime   TIMESTAMP,
    
    -- Evidencia
    has_gps             BOOLEAN DEFAULT FALSE,
    has_photo           BOOLEAN DEFAULT FALSE,
    has_signature       BOOLEAN DEFAULT FALSE,
    
    -- Auditor√≠a
    created_at          TIMESTAMP DEFAULT NOW(),
    updated_at          TIMESTAMP DEFAULT NOW()
);

-- √çndices para performance
CREATE INDEX idx_fact_visits_gestor ON fact_visits(gestor_id);
CREATE INDEX idx_fact_visits_territory ON fact_visits(territory_id);
CREATE INDEX idx_fact_visits_date ON fact_visits(date_id);
CREATE INDEX idx_fact_visits_executed ON fact_visits(executed_flag, date_id);
```

---

### üß± **Dimensi√≥n: `dim_gestor`**

```sql
CREATE TABLE dim_gestor (
    gestor_id               SERIAL PRIMARY KEY,
    
    -- Identificaci√≥n
    gestor_code             VARCHAR(50) UNIQUE NOT NULL,
    full_name               VARCHAR(200) NOT NULL,
    email                   VARCHAR(200),
    
    -- Caracter√≠sticas
    seniority_level         VARCHAR(20),  -- 'JUNIOR', 'SENIOR', 'EXPERT'
    contract_type           VARCHAR(20),  -- 'FULL_TIME', 'PART_TIME', 'CONTRACTOR'
    hire_date               DATE,
    
    -- Asignaci√≥n Territorial
    assigned_region_id      INTEGER,
    assigned_municipality_id INTEGER,
    
    -- M√©tricas Hist√≥ricas (SCD Type 2)
    historical_productivity DECIMAL(5,2),  -- Promedio hist√≥rico
    historical_quality      DECIMAL(5,2),  -- Score promedio
    total_visits_completed  INTEGER DEFAULT 0,
    
    -- SCD Type 2 (Slowly Changing Dimension)
    effective_date          DATE NOT NULL,
    expiration_date         DATE,
    is_current              BOOLEAN DEFAULT TRUE,
    
    -- Auditor√≠a
    created_at              TIMESTAMP DEFAULT NOW(),
    updated_at              TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_dim_gestor_current ON dim_gestor(is_current, gestor_code);
```

---

### üß± **Dimensi√≥n: `dim_territory`**

```sql
CREATE TABLE dim_territory (
    territory_id        SERIAL PRIMARY KEY,
    
    -- Jerarqu√≠a Territorial
    region_code         VARCHAR(10) NOT NULL,
    region_name         VARCHAR(100) NOT NULL,
    department_code     VARCHAR(10),
    department_name     VARCHAR(100),
    municipality_code   VARCHAR(10),
    municipality_name   VARCHAR(100),
    vereda_code         VARCHAR(10),
    vereda_name         VARCHAR(100),
    
    -- Caracter√≠sticas del Territorio
    criticality_level   VARCHAR(20),  -- 'LOW', 'MEDIUM', 'HIGH', 'CRITICAL'
    unit_density        INTEGER,      -- Unidades productivas por km¬≤
    population          INTEGER,
    area_km2            DECIMAL(10,2),
    
    -- Riesgo Base
    base_risk_score     DECIMAL(5,2), -- Score de riesgo hist√≥rico
    accessibility       VARCHAR(20),  -- 'EASY', 'MODERATE', 'DIFFICULT', 'VERY_DIFFICULT'
    
    -- Geolocalizaci√≥n
    latitude            DECIMAL(10,8),
    longitude           DECIMAL(11,8),
    
    -- Auditor√≠a
    created_at          TIMESTAMP DEFAULT NOW(),
    updated_at          TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_dim_territory_region ON dim_territory(region_code);
CREATE INDEX idx_dim_territory_municipality ON dim_territory(municipality_code);
```

---

### üß± **Dimensi√≥n: `dim_productive_unit`**

```sql
CREATE TABLE dim_productive_unit (
    unit_id             SERIAL PRIMARY KEY,
    
    -- Identificaci√≥n
    unit_code           VARCHAR(50) UNIQUE NOT NULL,
    unit_name           VARCHAR(200),
    
    -- Clasificaci√≥n
    unit_type           VARCHAR(50),  -- 'FARM', 'COOPERATIVE', 'ASSOCIATION', etc.
    size_category       VARCHAR(20),  -- 'SMALL', 'MEDIUM', 'LARGE'
    
    -- Riesgo
    base_risk_level     VARCHAR(20),  -- 'LOW', 'MEDIUM', 'HIGH'
    priority_level      INTEGER,      -- 1 (m√°s alta) - 5 (m√°s baja)
    
    -- Ubicaci√≥n
    territory_id        INTEGER REFERENCES dim_territory(territory_id),
    
    -- Estado
    is_active           BOOLEAN DEFAULT TRUE,
    last_visit_date     DATE,
    
    -- Auditor√≠a
    created_at          TIMESTAMP DEFAULT NOW(),
    updated_at          TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_dim_productive_unit_territory ON dim_productive_unit(territory_id);
CREATE INDEX idx_dim_productive_unit_active ON dim_productive_unit(is_active);
```

---

### üß± **Dimensi√≥n: `dim_date`**

```sql
CREATE TABLE dim_date (
    date_id             INTEGER PRIMARY KEY,  -- YYYYMMDD format
    
    -- Fecha completa
    full_date           DATE UNIQUE NOT NULL,
    
    -- Componentes de fecha
    day_of_week         INTEGER,      -- 1 (Lunes) - 7 (Domingo)
    day_of_month        INTEGER,
    day_of_year         INTEGER,
    week_of_year        INTEGER,
    month               INTEGER,
    quarter             INTEGER,
    year                INTEGER,
    
    -- Nombres
    day_name            VARCHAR(20),  -- 'Lunes', 'Martes', etc.
    month_name          VARCHAR(20),  -- 'Enero', 'Febrero', etc.
    quarter_name        VARCHAR(10),  -- 'Q1', 'Q2', etc.
    
    -- Flags especiales
    is_weekend          BOOLEAN,
    is_holiday          BOOLEAN,
    is_working_day      BOOLEAN,
    holiday_name        VARCHAR(100),
    
    -- Per√≠odos fiscales
    fiscal_year         INTEGER,
    fiscal_quarter      INTEGER,
    fiscal_month        INTEGER
);

-- Poblar dimensi√≥n de fechas (ejemplo: 10 a√±os)
INSERT INTO dim_date (date_id, full_date, day_of_week, day_of_month, ...)
SELECT 
    TO_CHAR(d, 'YYYYMMDD')::INTEGER,
    d,
    EXTRACT(ISODOW FROM d),
    EXTRACT(DAY FROM d),
    ...
FROM generate_series('2020-01-01'::DATE, '2030-12-31'::DATE, '1 day') AS d;
```

---

### üß± **Dimensi√≥n: `dim_alert`**

```sql
CREATE TABLE dim_alert (
    alert_id            SERIAL PRIMARY KEY,
    
    -- Clasificaci√≥n
    alert_type          VARCHAR(50) NOT NULL,  -- 'SECURITY', 'CLIMATE', 'OPERATIONAL', etc.
    severity_level      VARCHAR(20) NOT NULL,  -- 'CRITICAL', 'HIGH', 'MEDIUM', 'LOW'
    
    -- Descripci√≥n
    alert_category      VARCHAR(100),
    alert_description   TEXT,
    
    -- Resoluci√≥n
    is_resolvable       BOOLEAN DEFAULT TRUE,
    typical_resolution_days INTEGER,
    
    -- Auditor√≠a
    created_at          TIMESTAMP DEFAULT NOW()
);
```

---

## üìä **Hechos Derivados (Data Marts)**

### üéØ **Data Mart: `fact_daily_performance`**

Agregaci√≥n diaria para dashboards y reportes r√°pidos.

```sql
CREATE TABLE fact_daily_performance (
    performance_id          BIGSERIAL PRIMARY KEY,
    
    -- Dimensiones
    date_id                 INTEGER NOT NULL REFERENCES dim_date(date_id),
    gestor_id               INTEGER REFERENCES dim_gestor(gestor_id),
    territory_id            INTEGER REFERENCES dim_territory(territory_id),
    
    -- KPIs Operativos
    visits_planned          INTEGER DEFAULT 0,
    visits_executed         INTEGER DEFAULT 0,
    visits_failed           INTEGER DEFAULT 0,
    compliance_rate         DECIMAL(5,2),  -- % cumplimiento
    
    -- KPIs de Cobertura
    units_covered           INTEGER DEFAULT 0,
    coverage_rate           DECIMAL(5,2),  -- % cobertura
    
    -- KPIs de Calidad
    avg_visit_score         DECIMAL(5,2),
    visits_with_gps         INTEGER DEFAULT 0,
    visits_with_evidence    INTEGER DEFAULT 0,
    quality_score           DECIMAL(5,2),
    
    -- KPIs de Riesgo
    critical_alerts         INTEGER DEFAULT 0,
    preventive_alerts       INTEGER DEFAULT 0,
    risk_index              DECIMAL(5,2),
    
    -- Productividad
    avg_execution_time_min  DECIMAL(8,2),
    total_distance_km       DECIMAL(10,2),
    
    -- Auditor√≠a
    created_at              TIMESTAMP DEFAULT NOW(),
    
    UNIQUE(date_id, gestor_id, territory_id)
);

CREATE INDEX idx_fact_daily_perf_date ON fact_daily_performance(date_id);
CREATE INDEX idx_fact_daily_perf_gestor ON fact_daily_performance(gestor_id);
```

---

### üéØ **Data Mart: `fact_monthly_summary`**

Agregaci√≥n mensual para CEO Dashboard y OKRs.

```sql
CREATE TABLE fact_monthly_summary (
    summary_id              BIGSERIAL PRIMARY KEY,
    
    -- Per√≠odo
    year                    INTEGER NOT NULL,
    month                   INTEGER NOT NULL,
    territory_id            INTEGER REFERENCES dim_territory(territory_id),
    
    -- North Star KPI
    icoe_score              DECIMAL(5,2),  -- ICOE del mes
    icoe_trend              VARCHAR(10),   -- 'UP', 'DOWN', 'STABLE'
    
    -- KPIs Sat√©lite
    compliance_rate         DECIMAL(5,2),
    coverage_rate           DECIMAL(5,2),
    quality_score           DECIMAL(5,2),
    
    -- Riesgo
    avg_risk_index          DECIMAL(5,2),
    risk_level              VARCHAR(20),   -- 'LOW', 'MEDIUM', 'HIGH', 'CRITICAL'
    
    -- Productividad
    avg_productivity        DECIMAL(5,2),
    total_visits            INTEGER,
    active_gestors          INTEGER,
    
    -- Tendencias (vs mes anterior)
    icoe_change_pct         DECIMAL(5,2),
    compliance_change_pct   DECIMAL(5,2),
    coverage_change_pct     DECIMAL(5,2),
    
    -- Auditor√≠a
    created_at              TIMESTAMP DEFAULT NOW(),
    
    UNIQUE(year, month, territory_id)
);

CREATE INDEX idx_fact_monthly_summary_period ON fact_monthly_summary(year, month);
```

---

## üîÑ **Pipeline ETL/ELT**

### **Proceso de Carga Diaria**

```sql
-- Stored Procedure: Carga incremental diaria
CREATE OR REPLACE PROCEDURE sp_load_daily_facts()
LANGUAGE plpgsql
AS $$
BEGIN
    -- 1. Extraer visitas del d√≠a desde ODS
    INSERT INTO fact_visits (
        gestor_id,
        territory_id,
        productive_unit_id,
        date_id,
        planned_flag,
        executed_flag,
        execution_time_min,
        visit_score,
        has_gps,
        has_photo,
        has_signature,
        planned_datetime,
        executed_datetime
    )
    SELECT 
        dg.gestor_id,
        dt.territory_id,
        dpu.unit_id,
        TO_CHAR(v.created_at, 'YYYYMMDD')::INTEGER,
        v.status IN ('SCHEDULED', 'COMPLETED'),
        v.status = 'COMPLETED',
        EXTRACT(EPOCH FROM (v.verified_at - v.created_at))/60,
        CASE 
            WHEN v.latitude IS NOT NULL AND v.longitude IS NOT NULL THEN 100
            WHEN v.verified_at IS NOT NULL THEN 80
            ELSE 50
        END,
        v.latitude IS NOT NULL,
        v.verified_at IS NOT NULL,
        v.verified_at IS NOT NULL,
        v.created_at,
        v.verified_at
    FROM ods.visits v
    JOIN dim_gestor dg ON v.user_id = dg.gestor_code
    JOIN dim_productive_unit dpu ON v.productive_unit_id = dpu.unit_code
    JOIN dim_territory dt ON dpu.territory_id = dt.territory_id
    WHERE v.created_at::DATE = CURRENT_DATE - INTERVAL '1 day'
    ON CONFLICT DO NOTHING;
    
    -- 2. Actualizar fact_daily_performance
    INSERT INTO fact_daily_performance (
        date_id,
        gestor_id,
        territory_id,
        visits_planned,
        visits_executed,
        compliance_rate,
        avg_visit_score,
        visits_with_gps,
        quality_score,
        critical_alerts
    )
    SELECT 
        date_id,
        gestor_id,
        territory_id,
        SUM(CASE WHEN planned_flag THEN 1 ELSE 0 END),
        SUM(CASE WHEN executed_flag THEN 1 ELSE 0 END),
        ROUND(100.0 * SUM(CASE WHEN executed_flag THEN 1 ELSE 0 END) / 
              NULLIF(SUM(CASE WHEN planned_flag THEN 1 ELSE 0 END), 0), 2),
        AVG(visit_score),
        SUM(CASE WHEN has_gps THEN 1 ELSE 0 END),
        AVG(CASE WHEN has_gps AND has_photo AND has_signature THEN 100 ELSE 70 END),
        SUM(alert_count)
    FROM fact_visits
    WHERE date_id = TO_CHAR(CURRENT_DATE - INTERVAL '1 day', 'YYYYMMDD')::INTEGER
    GROUP BY date_id, gestor_id, territory_id
    ON CONFLICT (date_id, gestor_id, territory_id) 
    DO UPDATE SET
        visits_planned = EXCLUDED.visits_planned,
        visits_executed = EXCLUDED.visits_executed,
        compliance_rate = EXCLUDED.compliance_rate;
        
    COMMIT;
END;
$$;
```

---

## üß† **Ventajas del Star Schema**

‚úÖ **Queries R√°pidas**: Joins simples, √≠ndices optimizados
‚úÖ **BI Simple**: Herramientas como Power BI/Tableau se conectan f√°cilmente
‚úÖ **ML Listo**: Features ya preparadas para modelos
‚úÖ **Trazabilidad Total**: Input ‚Üí KPI ‚Üí Decisi√≥n
‚úÖ **Escalabilidad**: Particionamiento por fecha_id
‚úÖ **Historizaci√≥n**: SCD Type 2 en dimensiones cr√≠ticas

---

## ü§ñ **Predicci√≥n Avanzada con ML**

### **Principio ML Correcto**

> "No predecimos n√∫meros. Predecimos probabilidad de fallar."

---

### üéØ **Modelo 1: Predicci√≥n de Riesgo Territorial**

**Objetivo**: ¬øD√≥nde va a fallar la operaci√≥n?

**Target**:
```python
risk_event = 1  # Si ocurre alerta cr√≠tica en pr√≥ximos N d√≠as
```

**Features (Variables Predictoras)**:

```python
# Operaci√≥n (√∫ltimos 7/30 d√≠as)
- pct_visits_failed_7d
- pct_visits_failed_30d
- avg_delay_days
- recurring_alerts_count

# Gestor
- gestor_historical_score
- gestor_fatigue_index  # Visitas/d√≠a vs promedio
- gestor_recent_rotation  # Cambio reciente de zona

# Territorio
- territory_criticality_score
- territory_unit_density
- territory_base_risk

# Temporal
- is_rainy_season
- is_holiday_week
- day_of_week

# Tendencias
- visits_trend_7d  # Regresi√≥n lineal √∫ltimos 7 d√≠as
- alerts_trend_30d
```

**Modelo**:
```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import TimeSeriesSplit

# Preparaci√≥n de datos
X = df[features]
y = df['risk_event']

# Validaci√≥n temporal (no aleatoria)
tscv = TimeSeriesSplit(n_splits=5)

# Modelo
model = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    min_samples_split=50,
    class_weight='balanced',  # Importante: datos desbalanceados
    random_state=42
)

# Entrenamiento
model.fit(X_train, y_train)

# Predicci√≥n
risk_prob = model.predict_proba(X_test)[:, 1]
```

**Output**:
```json
{
  "municipality_id": "05001",
  "municipality_name": "Medell√≠n",
  "risk_probability": 0.78,
  "risk_level": "HIGH",
  "estimated_window_days": 14,
  "top_factors": [
    {"feature": "pct_visits_failed_7d", "importance": 0.32},
    {"feature": "gestor_fatigue_index", "importance": 0.24},
    {"feature": "territory_criticality_score", "importance": 0.18}
  ],
  "suggested_actions": [
    "Reasignar gestor de zona Norte",
    "Priorizar municipio en pr√≥ximas 72h",
    "Auditor√≠a preventiva programada"
  ]
}
```

---

### üéØ **Modelo 2: Predicci√≥n de Cobertura**

**Objetivo**: ¬øQu√© zonas perder√°n cobertura pronto?

**Target**:
```python
coverage_loss = 1  # Si cobertura cae bajo threshold en 30 d√≠as
```

**Features**:
```python
- visit_frequency_30d
- gestor_absences_count
- installed_capacity  # Gestores asignados
- actual_productivity  # Visitas/d√≠a real
- expected_productivity  # Visitas/d√≠a esperado
- territory_unit_count
- coverage_trend_30d
```

**Output**:
```json
{
  "territory_id": "05",
  "territory_name": "Antioquia",
  "current_coverage": 76,
  "projected_coverage_30d": 68,
  "risk_level": "MEDIUM",
  "confidence": 0.85,
  "recommendation": "Incrementar 2 gestores o reducir carga en 15%"
}
```

---

### üîÅ **Ciclo de Aprendizaje (MLOps Light)**

```
1. Predicci√≥n
   ‚Üì
2. Acci√≥n tomada (registrada)
   ‚Üì
3. Resultado real (observado)
   ‚Üì
4. Feedback loop
   ‚Üì
5. Reentrenamiento mensual
   ‚Üì
6. Validaci√≥n de mejora
   ‚Üì
7. Deployment autom√°tico (si mejora > 5%)
```

**Implementaci√≥n**:
```python
# ml_pipeline.py
class MLPipeline:
    def __init__(self):
        self.model = None
        self.feature_store = FeatureStore()
        
    def train(self, start_date, end_date):
        # 1. Extraer features del DW
        features = self.feature_store.get_features(start_date, end_date)
        
        # 2. Entrenar modelo
        self.model = self._train_model(features)
        
        # 3. Validar
        metrics = self._validate(features)
        
        # 4. Si mejora, guardar
        if metrics['f1_score'] > self.current_best_f1:
            self._save_model(self.model, metrics)
            
    def predict(self, territory_id, horizon_days=30):
        # 1. Obtener features actuales
        current_features = self.feature_store.get_current_features(territory_id)
        
        # 2. Predecir
        risk_prob = self.model.predict_proba([current_features])[0][1]
        
        # 3. Generar explicaci√≥n
        explanation = self._explain_prediction(current_features)
        
        return {
            'probability': risk_prob,
            'level': self._classify_risk(risk_prob),
            'explanation': explanation
        }
```

---

## üéõÔ∏è **Integraci√≥n con Dashboards**

### **CEO View**

```sql
-- Query para CEO Dashboard: Zonas con riesgo en 30 d√≠as
SELECT 
    dt.region_name,
    dt.municipality_name,
    mp.risk_probability,
    mp.risk_level,
    mp.estimated_window_days,
    mp.suggested_actions
FROM ml_predictions mp
JOIN dim_territory dt ON mp.territory_id = dt.territory_id
WHERE mp.prediction_type = 'TERRITORIAL_RISK'
  AND mp.risk_level IN ('HIGH', 'CRITICAL')
  AND mp.prediction_date = CURRENT_DATE
ORDER BY mp.risk_probability DESC
LIMIT 10;
```

### **Coordinador View**

```sql
-- Query para Alertas Predictivas
SELECT 
    dg.full_name AS gestor,
    dt.municipality_name,
    mp.risk_probability,
    mp.top_factors,
    mp.suggested_actions
FROM ml_predictions mp
JOIN dim_gestor dg ON mp.gestor_id = dg.gestor_id
JOIN dim_territory dt ON mp.territory_id = dt.territory_id
WHERE mp.prediction_type = 'COVERAGE_LOSS'
  AND mp.risk_level = 'MEDIUM'
  AND dg.assigned_region_id = :coordinator_region_id
ORDER BY mp.risk_probability DESC;
```

---

## üß† **Transparencia (Clave Enterprise)**

Cada predicci√≥n incluye:

1. **Variables que m√°s influyeron** (Feature Importance)
2. **Nivel de confianza** (Probability Score)
3. **Recomendaci√≥n sugerida** (Actionable Insight)
4. **Datos hist√≥ricos** (Baseline Comparison)

**Ejemplo de Explicaci√≥n**:
```json
{
  "prediction_id": "PRED-2026-001",
  "municipality": "Medell√≠n",
  "risk_probability": 0.78,
  "confidence": 0.85,
  "top_factors": [
    {
      "factor": "Visitas fallidas √∫ltimos 7 d√≠as",
      "value": "32%",
      "importance": 0.32,
      "threshold": "15%",
      "status": "ABOVE_THRESHOLD"
    },
    {
      "factor": "Fatiga operativa del gestor",
      "value": "1.8x promedio",
      "importance": 0.24,
      "threshold": "1.2x",
      "status": "ABOVE_THRESHOLD"
    }
  ],
  "historical_accuracy": {
    "last_30_predictions": 0.82,
    "precision": 0.79,
    "recall": 0.85
  },
  "suggested_actions": [
    "Reasignar gestor senior a zona Norte",
    "Reducir carga operativa en 20%",
    "Programar auditor√≠a en 72h"
  ]
}
```

---

## üèÅ **Resultado Final**

Con **Star Schema + ML**:

‚úÖ **Datos confiables**: Una sola fuente de verdad
‚úÖ **KPIs explicables**: Trazabilidad completa
‚úÖ **Alertas anticipadas**: Predicci√≥n a 30 d√≠as
‚úÖ **Simulaciones realistas**: What-If basado en datos reales
‚úÖ **Decisiones defendibles**: Ante Board/Inversionistas
‚úÖ **Escalabilidad real**: Millones de registros sin degradaci√≥n

---

## üìä **Stack Tecnol√≥gico Recomendado**

### **Data Warehouse**
- **PostgreSQL 15+** con extensiones:
  - `pg_partman` (particionamiento autom√°tico)
  - `timescaledb` (series temporales)
  - `pg_stat_statements` (monitoreo de queries)

### **ETL/ELT**
- **Apache Airflow** (orquestaci√≥n)
- **dbt** (transformaciones SQL)
- **Airbyte** (conectores a fuentes)

### **ML**
- **Python 3.11+**
- **scikit-learn** (modelos baseline)
- **XGBoost** (modelos avanzados)
- **MLflow** (tracking de experimentos)
- **SHAP** (explicabilidad)

### **BI**
- **Metabase** (open source, f√°cil)
- **Power BI** (enterprise)
- **Superset** (Apache, flexible)

---

**Versi√≥n**: 1.0.0  
**√öltima actualizaci√≥n**: 2026-01-29  
**Estado**: üìã **Especificaci√≥n Completa - Listo para Implementar**

---

## üöÄ **Pr√≥ximos Pasos**

1. **Fase 1**: Implementar Star Schema (2 semanas)
2. **Fase 2**: Pipeline ETL diario (1 semana)
3. **Fase 3**: Data Marts agregados (1 semana)
4. **Fase 4**: Modelo ML de Riesgo (2 semanas)
5. **Fase 5**: Integraci√≥n con Dashboards (1 semana)

**Total estimado**: 7-8 semanas para plataforma completa de inteligencia operativa.
